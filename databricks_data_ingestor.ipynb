{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from databricks import sql \n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_with_retries(cursor, query, max_retries=5, delay=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            return\n",
    "        except sql.exc.RequestError as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"Query failed (Attempt {retries+1}/{max_retries}): {error_message}\")  \n",
    "            \n",
    "            if \"429\" in error_message or \"503\" in error_message:  # Rate limit or service unavailable\n",
    "                retries += 1\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(f\"Max retries exceeded for query execution. Last error: {error_message}\")\n",
    "\n",
    "def escape_sql_value(value):\n",
    "    if value is None or str(value).strip().lower() in {\"null\", \"\"}:\n",
    "        return \"NULL\"\n",
    "    elif isinstance(value, (int, float)):  \n",
    "        return str(value)  # Keep numbers unquoted\n",
    "    else:\n",
    "        value = str(value)  \n",
    "        value = value.replace(\"'\", \"''\").replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "        return f\"'{value}'\"\n",
    "\n",
    "def process_chunk(chunk_number, chunk, schema, table_name, connection_details):\n",
    "    connection = sql.connect(**connection_details)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    values_list = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        values = [escape_sql_value(val) for val in row]\n",
    "        values_list.append(f\"({', '.join(values)})\")\n",
    "\n",
    "    if values_list:\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {schema}.{table_name}\n",
    "        VALUES {', '.join(values_list)}\n",
    "        \"\"\"\n",
    "        execute_with_retries(cursor, insert_query)\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def upload_file_to_databricks(df, table_name, schema, chunk_size, num_processes):\n",
    "    # Ensure all data is converted to strings and replace NaN with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    connection = sql.connect(**connection_details)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    columns = [f\"`{col}` STRING\" for col in df.columns]\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {schema}.{table_name} \n",
    "        USING delta \n",
    "        TBLPROPERTIES (\n",
    "            'delta.columnMapping.mode' = 'name',\n",
    "            'delta.minReaderVersion' = '2',\n",
    "            'delta.minWriterVersion' = '5'\n",
    "        )\n",
    "        AS SELECT {', '.join([f\"CAST(NULL AS STRING) AS `{col}`\" for col in df.columns])} WHERE 1=0\n",
    "    \"\"\"\n",
    "    execute_with_retries(cursor, create_table_query)\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Split data into chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Multiprocessing for parallel insertion\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            result = pool.apply_async(process_chunk, args=(i, chunk, schema, table_name, connection_details))\n",
    "            results.append(result)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks configuration\n",
    "connection_details = {\n",
    "    \"server_hostname\": \"<DATABRICKS_HOSTNAME>\",\n",
    "    \"http_path\": \"<DATABRICKS_HTTP_PATH>\",\n",
    "    \"access_token\": \"<DATABRICKS_ACCESS_TOKEN>\"\n",
    "}\n",
    "\n",
    "file_path = \"<LOCAL_FILE_PATH>\"\n",
    "df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "table_name = \"<TABLE_NAME>\"\n",
    "\n",
    "try:\n",
    "    upload_file_to_databricks(df, table_name, schema=\"<SCHEMA_NAME>\", chunk_size=5000, num_processes=10)  # Control parallelism here\n",
    "except Exception as e:\n",
    "    print(f\"Failed to upload file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
